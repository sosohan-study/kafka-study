# AWS를 이용한 EC2 구성 
# 카프카 클러스터 구성 

스텐드얼론 환경이 아닌 클러스터 환경을 기본으로 구성한다.    
AWS의 메니저드 서비스가 아닌 EC2 인스턴스 위에 카프카 클러스터를 직접 설치!    

## 앤서블 설치  

카프카를 구성하는데 어려움이 있어서 자동화 도구인 **앤서블(Ansible)** 를 이용하여 주키퍼와 카프카를 설치한다.     
앤서블은 다수의 서버를 대상으로 설정 관리, 애플리케이션 배포 등을 코드로 관리할 수 있도록 도움을 주는 도구다.   

**EC2**   
```console
sudo amazon-linux-extras install -y ansible2
```

## 깃설치 클론 

**깃설치**   
```console
sudo yum install -y git
```
* 깃이 없을 수 잇으므로 설치해주자 

**클론**
```console
git clone https://github.com/kwj1270/kafka2.git
```
* 원본 레포를 해치고 싶지 않아서 포크한후 클론했다.  

## 보안키 설정 
  
인스턴스에서 다른 인스턴스로 접속이 가능하려면, 그에 맞는 pem 키가 있어야한다.         
로컬에서 인스턴스로 접근할때는 키가 있어서 괜찮지만, 인스턴스에서 인스턴스는 그렇지 않다.        
이를 위해 배포 서버에서 pem 키를 가지고 다른 인스턴스에 접근할 수 있도록 설정해주어야한다.      
    
방법은 간단하다. 로컬에서 배포 서버에 pem 키를 보내면 된다.(ssl 방식이므로 scp 이용)     

**로컬**
```console
scp -i kafka-test.pem kafka-test.pem ec2-user@퍼블릭IP:~
```
* 로컬에서 EC2로 서버키를 배포한다.     
 
**배포서버**
```console
chmod 600 kafka-test.pem
ssh-agent bash
ssh-add kafka-test.pem
```
* 배포 서버는 키를 등록한다.       
* 이후부터는 자유롭게 인스턴스들을 돌아다닐 수 있다.    
  
### 배포서버 공개키로 설정   
  
그러나 위와 같은 방법은 배포서버에 재접속할 때마다 설정이 초기화가 된다.      
이를 해결하기 위해서 배포서버에서 별도로 ssh 공개 키를 생성하는 방식도 추천한다.    
  
1. 배포 서버에서 공개 키 생성  
2. 공개 키 내용을 접속하고자 하는 서버에 복사    
3. 배포 서버에서 다른 서버로 비밀번호 없이 접속  

**배포 서버**
```console
ssh-keygen
```  
* 키를 누르면 설정 관련 내용이 나오는데 디폴트값(엔터)로만 설정한다.       
* 설정이 완료되면, 기본값으로 `/home/ec2-user/.ssh/id_rsa.pub`        
     
**출력**   
```console
cat /home/ec2-user/.ssh/id_rsa.pub
```
```console
ssh-rsa AAAAB3Nzapqaihlakugfqueogaebfyavdknsbkhvcmzsvmcn 
asfdasuofbiqevfiqfqgfuSUOGDWVQjashdohsoshblDosuqgobdsljd  
asdosoafdISHDKfnSpojfpjlseiorhlkszldkZSPIDPsdojpaeropjad   
```
해당 키 내용을 복사한 후에, 나머지 서버들에 authorized_keys 에 들어가서 붙여넣으면 된다.         
    
```console     
vi /home/ec2-user/.ssh/authoried_keys          
chmod 600 .ssh/authorized_keys        
```

불여넣기를 한 다음, authorized_keys 파일의 권한은 600이 되어야 하므로 chmod를 이용해서 변경해주자.          
이후부터는 별도의 비밀번호 작업없이도 배포서버에서 다른 인스턴스들로 이동이 가능하다.        
 
## 주키퍼 설치       
            
배포서버의 클론 디렉토리로 이동한다.               
     
* 배포된 프로젝트의 ansible_playbook 디렉토리로 이동한 후,                  
* 앤서블 명령어인 ansible-playbook 을 써서 hosts 파일에 지정된            
  peter-zk01, peter-zk02, peter-zk03 서버에 모두 주키퍼를 설치한다.        
  
**hosts 파일**
```
[zkhosts]
peter-zk01.foo.bar
peter-zk02.foo.bar
peter-zk03.foo.bar

[kafkahosts]
peter-kafka01.foo.bar
peter-kafka02.foo.bar
peter-kafka03.foo.bar

[kerberoshosts]
```
  
**zookeeper.yml**
```yml
---
- hosts: zkhosts
  become: true
  connection: ssh
  roles:
    - common
    - zookeeper
```          
hosts 파일에 있는 목록에 의해 해당하는 도메인을 가진 서버에 설치를 한번에 해준다.           
  
**주키퍼 설치**   
```console  
cd kafka2/chapter2/ansible_playbook/      
ansible-playbook -i hosts zookeeper.yml 
```
* 단 실행전에 해당 서버에 접속해야한다.(yml에 등록된 도메인으로)   
   
```
[출력]
PLAY RECAP ***********************************************************************************************************
peter-zk01.foo.bar         : ok=18   changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
peter-zk02.foo.bar         : ok=18   changed=14   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1
peter-zk03.foo.bar         : ok=18   changed=14   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1
```  

주키퍼 설치가 완료되었다면, 주키퍼가 제대로 실행되고 있는지 확인하자.     
주키퍼는 리눅스의 systemd를 통해서 관리되도록 앤서블에 포함했으므로,     
모든 주키퍼 서버 중 1대에 접속해서 systemctl 명령어를 이용해 확인하면 된다.  
  
앤서블 플레이북을 하면, 실행되는 동일한 DN 서버에 주키퍼가 설치된다.       
즉, 전체 7개중 주키퍼 인스턴스중 하나에 접속해서 확인하면 된다.           
 
```console  
[주키퍼 서버] sudo systemctl status zookeeper-server      
```    
```console
[ec2-user@ip-172-31-44-217 ~]$ sudo systemctl status zookeeper-server
● zookeeper-server.service - zookeeper-server
   Loaded: loaded (/etc/systemd/system/zookeeper-server.service; enabled; vendor preset: disabled)
   Active: active (running) since 월 2022-01-03 15:08:11 KST; 1s ago
  Process: 11414 ExecStart=/usr/local/zookeeper/bin/zkServer.sh start (code=exited, status=0/SUCCESS)
 Main PID: 11431 (java)
   CGroup: /system.slice/zookeeper-server.service
           └─11431 java -Dzookeeper.log.dir=/usr/local/zookeeper/bin/../logs ...

 1월 03 15:08:10 ip-172-31-44-217.ap-northeast-2.compute.internal systemd[1]: ...
 1월 03 15:08:10 ip-172-31-44-217.ap-northeast-2.compute.internal systemd[1]: ...
 1월 03 15:08:10 ip-172-31-44-217.ap-northeast-2.compute.internal systemd[1]: ...
 1월 03 15:08:10 ip-172-31-44-217.ap-northeast-2.compute.internal zookeeper-server[11414]: ...
 1월 03 15:08:10 ip-172-31-44-217.ap-northeast-2.compute.internal zookeeper-server[11414]: ...
 1월 03 15:08:10 ip-172-31-44-217.ap-northeast-2.compute.internal zookeeper-server[11414]: ...
 1월 03 15:08:11 ip-172-31-44-217.ap-northeast-2.compute.internal systemd[1]: ...
Hint: Some lines were ellipsized, use -l to show in full.  
```  
 
출력 내용중 Active: active(running)이 확인되면 정상적으로 실행되고 있는 상태다.         
만약, Active: failed로 뜬다면 실행이 안 되는 상황이므로, 다시 한번 앤서블을 이용해서 설치해야한다.      
정상적이라면 나머지 주키퍼 서버들도 다시 테스트해보자      
    
## 카프카 설치 
  
주키퍼 설치가 완료되었다면, 카프카를 설치할 차례다.        
주키퍼 설치 방식과 동일하게, 앤서블 명령어인 ansible-playbook 을 사용하고     
-i 옵션으로 hosts 파일에 지정된 peter-kafka01, peter-kafka02, peter-kafka03 서버에 모두 카프카를 설치한다.  

```console
cd kafka2/chapter2/ansible_playbook/      
ansible-playbook -i hosts kafka.yml
```
* 단 실행전에 해당 서버에 접속해야한다.(yml에 등록된 도메인으로)   
   
```console

PLAY RECAP ***********************************************************************************************************
peter-kafka01.foo.bar      : ok=15   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1
peter-kafka02.foo.bar      : ok=15   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1
peter-kafka03.foo.bar      : ok=15   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1
```

이제 실습을 위한 주키퍼와 카프카 클러스터 환경이 구성이 완료되었다.         
주키퍼와 마찬가지로 카프카 서버 중 1대에 접근해 프로세스가 잘 실행됐는지 확인해보자.     

```console  
[카프카 서버] sudo systemctl status kafka-server   
```   
```console
[ec2-user@ip-172-31-46-193 ~]$ sudo systemctl status kafka-server
● kafka-server.service - kafka-server
   Loaded: loaded (/etc/systemd/system/kafka-server.service; disabled; vendor preset: disabled)
   Active: active (running) since 월 2022-01-03 15:13:55 KST; 6s ago
 Main PID: 11858 (java)
   CGroup: /system.slice/kafka-server.service
           └─11858 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:M...

 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.client.StaticHostProvider$1.getAllByName(S...a:92)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.client.StaticHostProvider.resolve(StaticHo...:147)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.client.StaticHostProvider.next(StaticHostP...:375)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1137)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: [2022-01-03 15:14:00,676] WARN Session 0x0 for server peter-zk03.f...Cnxn)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: java.lang.IllegalArgumentException: Unable to canonicalize address...vable
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.SaslServerPrincipal.getServerPrincipal(Sas...a:71)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.SaslServerPrincipal.getServerPrincipal(Sas...a:39)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientC...1087)
 1월 03 15:14:00 ip-172-31-46-193.ap-northeast-2.compute.internal kafka-server[11858]: at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1139)
Hint: Some lines were ellipsized, use -l to show in full.
```
주키퍼와 동일하게 출력 내용 중 Active: active(running)이 확인되면, 정상적으로 실행된 것이다.       
나머지 카프카 서버에도 접근해 확인하자.    
